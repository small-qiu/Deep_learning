{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "PDE.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/small-qiu/Deep_learning/blob/master/PDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkEQbs8AJwWH",
        "colab_type": "text"
      },
      "source": [
        "##### 深度学习求解高微分方程"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P1rQ_TbJwWI",
        "colab_type": "text"
      },
      "source": [
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJeCw7kaJwWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61
        },
        "outputId": "90d76f65-2fac-4527-ac3c-96fb9e0be8b3"
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "     raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIDPN9CBJwWP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "a1cec9fe-51bc-48dd-8d7b-22e2672baf4e"
      },
      "source": [
        "import time  \n",
        "import math\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average   # 移动平均\n",
        "from scipy.stats import multivariate_normal as normal            # 产生正态分布随机数\n",
        "from tensorflow.python.ops import control_flow_ops               #用于控制流  \n",
        "from tensorflow import random_normal_initializer as norm_init    #生成具有正态分布的张量的初始化器\n",
        "from tensorflow import random_uniform_initializer as unif_init   #生成具有均匀分布的张量的初始化器\n",
        "from tensorflow import constant_initializer as const_init        #生成具有常量值的张量的初始化器\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class SolveAllenCahn (object):\n",
        "    \"\"\" The fully-connected neural network model .全连接\"\"\"  \n",
        "    def __init__ (self,sess):\n",
        "        self.sess = sess     # 会话\n",
        "        # parameters for the PDE\n",
        "        self.d = 100      # 数据的维度\n",
        "        self.T = 0.3      # 每一条路径的时间长度\n",
        "        # parameters for the algorithm\n",
        "        self.n_time = 20     # 有20个网络构成\n",
        "        self.n_layer = 4     # 神经网络的层数\n",
        "        self.n_neuron = [self.d ,self.d +10 ,self.d +10, self.d ]    # 各层神经元的个数，对应输入、隐藏层1、隐藏层2、输出\n",
        "        self.batch_size = 64      # 一次用到个路径计算，64*4=256\n",
        "        self.valid_size = 256     # 256个蒙特卡洛样本（路径）\n",
        "        self.n_maxstep = 4000     #迭代步数\n",
        "        self.n_displaystep = 1000     # 每100步print一次\n",
        "        self.learning_rate = 5e-4    # 学习率\n",
        "        self.Yini = [0.3,0.6]       # 初始值Y0的最大最小值\n",
        "        # some basic constants and variables\n",
        "        self.h = (self.T +0.0)/self.n_time    # 每一个路径中数据时间间隔，delta t\n",
        "        self.sqrth = math.sqrt(self.h)    # 根号delta t，后面用于计算\n",
        "        self.t_stamp = np.arange(0,self.n_time)*self.h  # 时间戳，累计的时间\n",
        "        self._extra_train_ops = []  # batch移动平均值操作，其中需要额外训练的beta和gamma\n",
        "\n",
        "    def train(self):\n",
        "        # 主要函数，用于神经网络的训练\n",
        "        start_time = time.time()   # 起始时间\n",
        "        # train operations创建新的tensorflow变量,name='global_step'，用产量生成器\n",
        "        self.global_step = \\\n",
        "            tf.get_variable('global_step', [] ,\n",
        "                              initializer = tf.constant_initializer(1),\n",
        "                              trainable = False,dtype = tf.int32 )   # 没有添加到要训练的变量列表，计步器\n",
        "        trainable_vars = tf.trainable_variables()  # 查看可训练的变量\n",
        "        grads = tf.gradients(self.loss,trainable_vars)  # loss可训练变量的梯度\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)    # 梯度优化器\n",
        "        apply_op = \\\n",
        "            optimizer.apply_gradients(zip(grads,trainable_vars) ,    # 将梯度用来更新trainable_vars列表中的东西\n",
        "                                          global_step = self.global_step)   # 更新梯度和迭代次数\n",
        "        \n",
        "        train_ops = [apply_op] + self._extra_train_ops   # 添加操作，相当于list1.extand(list2)\n",
        "        self.train_op = tf.group(* train_ops)   # tf.group(*train_ops)组合*train_ops的操作\n",
        "        \n",
        "        self.loss_history = []   # 用于记录loss值\n",
        "        self.init_history = []   # 用于记录Y0的值\n",
        "        \n",
        "        # for validation,256条蒙特卡洛做验证集\n",
        "        dW_valid , X_valid = self.sample_path(self.valid_size)   # 生成数据\n",
        "        feed_dict_valid = { self.dW : dW_valid,   # 喂数据给buildmodel中的占位符\n",
        "                            self.X : X_valid,\n",
        "                            self.is_training: False }   # 不列入迭代范围\n",
        "        # initialization\n",
        "        step = 1\n",
        "        self.sess.run (tf.global_variables_initializer())  # 初始化全局变量\n",
        "        \n",
        "        # 运行框架\n",
        "        temp_loss = self.sess.run(self.loss ,\n",
        "                                  feed_dict = feed_dict_valid )  # 计算损失\n",
        "        \n",
        "        temp_init = self.Y0.eval()[0] # # 取出值，Y0是二维张量\n",
        "        self.loss_history.append(temp_loss)  # 记录loss\n",
        "        self.init_history.append(temp_init)  # 记录Y0\n",
        "        print(\"step : %5u , loss : %.4e , \" % \\\n",
        "                (0 ,temp_loss ) + \\\n",
        "                \"Y0 : % .4e , runtime : %4u \" % \\\n",
        "                (temp_init, time.time()-start_time + self.t_bd))   # 打印step=0的状态\n",
        "        \n",
        "        # begin sgd iteration，0-4000步\n",
        "        for _ in range (self.n_maxstep +1):   \n",
        "            step = self.sess.run (self.global_step)\n",
        "            dW_train,X_train = self.sample_path(self.batch_size)  # 生成数据\n",
        "            self.sess.run(self.train_op,\n",
        "                          feed_dict ={self.dW : dW_train ,   # 喂数据给buildmodel中的占位符\n",
        "                                      self.X : X_train ,\n",
        "                                      self.is_training : True })\n",
        "            if step % self.n_displaystep == 0:   # 每100步用验证集测试一下损失和Y0的值\n",
        "                temp_loss = self.sess.run(self.loss ,\n",
        "                                          feed_dict = feed_dict_valid)\n",
        "                temp_init = self.Y0.eval()[0]    # 取出值\n",
        "                self.loss_history.append(temp_loss)    # 损失值\n",
        "                self.init_history.append(temp_init )   # Y0值，Y0是二维张量\n",
        "                print(\"step : % 5u , loss : %.4e , \" % \\\n",
        "                        ( step , temp_loss ) + \\\n",
        "                        \" Y0 : % .4e , runtime : %4u \" % \\\n",
        "                        (temp_init , time.time() - start_time + self.t_bd ))\n",
        "            step += 1\n",
        "        end_time = time.time()  # 训练结束的总时间\n",
        "        print(\" running time : % .3f s \" % \\\n",
        "                ( end_time - start_time + self.t_bd ))\n",
        "\n",
        "    def build(self):\n",
        "        # build the whole network by stacking subnetworks，构架大网络\n",
        "        start_time = time.time () \n",
        "        # dW、X、is_training的占位符，为什么是None,因为一次计算一个batch\n",
        "        self.dW = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time ] ,name = 'dW')   # None*100*20\n",
        "        self.X = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time +1] ,name = 'X')   # None*100*20\n",
        "        self.is_training = tf.placeholder (tf.bool)\n",
        "        \n",
        "        # 初始化Y0\\Z0\n",
        "        self.Y0 = tf.Variable(tf.random_uniform([1] ,                      # u0初始化,一个维度一个值  \n",
        "                                                minval = self.Yini [0] ,   # 最小值0.3  \n",
        "                                                maxval = self.Yini [1] ,   # 最大值0.6\n",
        "                                                dtype = tf.float32 ));\n",
        "        self.Z0 = tf.Variable (tf.random_uniform ([1,self.d] ,    # u梯度的初始值，一个1*d 向量\n",
        "                                                minval = -.1 ,   # 最小值\n",
        "                                                maxval =.1 ,    # 最大值\n",
        "                                                dtype = tf.float32 ))\n",
        "        self.allones = \\\n",
        "             tf.ones(shape = tf.stack([ tf.shape(self.dW)[0],1]) ,   # tf.shape(self.dW)[0]=len(batch),shape=(batch,1)\n",
        "                         dtype = tf.float32 )                        # 作用，批量（batch）产生初始值\n",
        "\n",
        "        Y = self.allones * self.Y0  # 初始的Y作为输入,每一个batch都赋予相同的初始Y值，切Y是一个(batch,1)二维矩阵[[],[],..,]\n",
        "        Z = tf.matmul(self.allones, self.Z0 )  # 初始的Z做为输出，作用和Y相同，但是由于Z是一个向量所以要乘积（batch,d）矩阵\n",
        "        \n",
        "        \n",
        "        with tf.variable_scope('forward'):   # 前向\n",
        "            for t in range(0,self.n_time -1):  # 前N-1个xt的网络\n",
        "                    # 计算过程来源于递推公式\n",
        "                    Y = Y - self.f_tf(self.t_stamp[t] ,       # 时间戳，累计的时间值\n",
        "                                    self.X[:,:,t],Y,Z)* self.h   # Y.shape=(batch,1) \n",
        "                    Y = Y + tf.reduce_sum(Z * self.dW[:,:,t],1, # 矩阵点乘。\n",
        "                                       keep_dims = True )      # 得到中间时刻的输出。\n",
        "                    Z = self._one_time_net(self.X[:,:,t +1] ,  # 得到“u梯度”-- 由神经网络训练而来的。\n",
        "                                       str(t +1))/self.d      # str(t +1) 名字，why /self.d ?\n",
        "            # terminal time，因为最后一刻的Y不用神经网络了\n",
        "            Y = Y - self.f_tf(self.t_stamp[self.n_time -1] ,   # 为什么-1,下标从0开始的，最后一刻有所不同。\n",
        "                                  self.X[:,:,self.n_time -1] , # \n",
        "                                  Y,Z)* self.h\n",
        "            Y = Y + tf.reduce_sum(Z * self.dW [:,:,self.n_time -1] , 1 ,\n",
        "                                      keep_dims = True )   # 最后一次的Y,也即最终输出\n",
        "            term_delta = Y - self.g_tf(self.T,\n",
        "                                   self.X[:,:,self.n_time]) # 损失函数公式\n",
        "            self.clipped_delta = \\\n",
        "                  tf.clip_by_value(term_delta ,-50.0 , 50.0)  #将一个张量中的数值限制在一个范围之内，大于为上界，小于为下界\n",
        "            self.loss = tf.reduce_mean(self.clipped_delta**2)  #计算损失，来源于损失函数\n",
        "        self.t_bd = time.time() - start_time  # 生成网络的时间\n",
        "\n",
        "    def sample_path(self, n_sample):\n",
        "        # 产生路径，生成（xt,(wt-wt-1)）\n",
        "        dW_sample = np.zeros([n_sample,self.d,self.n_time])   # 样本数、维度、时间长度\n",
        "        X_sample = np.zeros([n_sample,self.d,self.n_time +1])\n",
        "        for i in range(self.n_time):    # 一次生成一列\n",
        "            dW_sample [:,:,i] = \\\n",
        "               np.reshape(normal.rvs(mean = np.zeros(self.d) ,  # 该函数相当于np.random.normal()\n",
        "                                     cov =1 ,   # 为什么不是std=1 ?\n",
        "                                     size = n_sample)* self.sqrth ,   # 根号delta t，W(t)-W(s)独立于的W(r),且是期望为0方差为t-s\n",
        "                          (n_sample,self.d))\n",
        "            X_sample[:,:,i +1] = X_sample[:,:,i] + \\\n",
        "                                   np.sqrt(2)*dW_sample[:,:,i]   # 来自公式\n",
        "        return dW_sample, X_sample\n",
        "\n",
        "    def f_tf(self,t,X,Y,Z ):\n",
        "        # nonlinear term\n",
        "        return Y - tf.pow(Y,3)\n",
        "\n",
        "    def g_tf(self,t,X):\n",
        "        # terminal conditions\n",
        "        return 0.5/(1 + 0.2* tf.reduce_sum(X **2,1,keep_dims = True ))\n",
        "\n",
        "    def _one_time_net(self , x ,name ):\n",
        "        # 一个batch在t时刻的网络构架，输出梯度\n",
        "        with tf.variable_scope(name):\n",
        "            x_norm = self._batch_norm(x , name = 'layer0_normal')  # 对batch标准化，作为输入\n",
        "            layer1 = self._one_layer(x_norm , self.n_neuron [1] ,   # 隐藏层1输入输出input(batch,d),output(batch，d+10)\n",
        "                                      name = 'layer1')\n",
        "            layer2 = self._one_layer(layer1,self.n_neuron[2] ,  # 隐藏层2 input(batch,d+10),output(batch,d+10)\n",
        "                                      name = 'layer2')\n",
        "            z = self._one_layer(layer2 , self.n_neuron [3] , #  输出层，不加relu函数做激活input(batch,d+10),output(baatch,d)\n",
        "                                     activation_fn = None , name = 'final')\n",
        "        return z\n",
        "\n",
        "    def _one_layer(self , input_ , out_sz ,\n",
        "                   activation_fn = tf.nn.relu ,\n",
        "                   std =5.0 , name = 'linear'):\n",
        "        # 一个层里面的计算操作,一个层的输入，返回一个层的输出0，被_one_time_net调用用于构造一个时刻的神经网络\n",
        "        with tf.variable_scope(name):\n",
        "            shape = input_.get_shape().as_list()  # list(输入的维度)\n",
        "            w = tf.get_variable('Matrix',     # get_variable它一定是和tf.variable_scope()共同使用的\n",
        "                                [shape[1], out_sz] ,tf.float32,    # 输入维度和输出维度\n",
        "                                norm_init(stddev = \\\n",
        "                                          std / np.sqrt(shape[1]+ out_sz )))  # 生成权重矩阵\n",
        "            hidden = tf.matmul(input_ ,w )  # 矩阵乘积，隐藏计算中间结果\n",
        "            hidden_bn = self._batch_norm(hidden, name = 'normal')  # batch标准化\n",
        "        if activation_fn != None :\n",
        "            return activation_fn(hidden_bn)  # 激活函数\n",
        "        else :\n",
        "            return hidden_bn  #不加激活函数,线性\n",
        "\n",
        "    def _batch_norm(self , x , name ):\n",
        "        \"\"\" Batch normalization \"\"\" # beta、gamma需要训练，第三类参数来源,一次标准化需要2列参数\n",
        "        with tf.variable_scope(name):\n",
        "            params_shape = [x.get_shape()[ -1]]   # [d,d+10,d+10,d]，第一个维度是batch\n",
        "            beta = tf.get_variable('beta', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         norm_init(0.0 , stddev =0.1 ,\n",
        "                                         ))\n",
        "            gamma = tf.get_variable( 'gamma', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         unif_init (0.1,0.5 ,\n",
        "                                          ))\n",
        "            mv_mean = tf.get_variable('moving_mean' ,   # 由于每次的batch不同，所以用moving_mean来改进mean\n",
        "                                         params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         const_init (0.0) ,\n",
        "                                         trainable = False )\n",
        "            mv_var = tf.get_variable('moving_variance' ,\n",
        "                                        params_shape ,\n",
        "                                        tf.float32 ,\n",
        "                                        const_init(1.0) ,\n",
        "                                        trainable = False )\n",
        "            \n",
        "            # These ops will only be preformed when training\n",
        "            mean ,variance = tf.nn.moments(x ,[0] , name = 'moments')#需要标准化的中心维度,[0]表示batch,求64个数据的均值方差\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_mean , mean , 0.99))  # 下面详解\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_var , variance , 0.99))\n",
        "            \n",
        "            mean,variance = \\\n",
        "                control_flow_ops.cond(self.is_training ,            # control_flow_ops.cond控制执行流，第一个为条件\n",
        "                                     lambda :( mean , variance ) , # 条件True时执行，train时，需要进行重新求均值方差\n",
        "                                     lambda :( mv_mean , mv_var )) # 条件False时执行,test时，直接调用最后一次的平滑值\n",
        "            \n",
        "            y = tf.nn.batch_normalization (x , mean , variance ,\n",
        "                                           beta , gamma , 1e-6)   \n",
        "            # 上面一步的操作相当于:  \n",
        "            # y = (y - mean)/tf.sqrt(variance+1e-6)  # 1e-6 epslion\n",
        "            # y = y * gamma + beta\n",
        "            \n",
        "            # 确保标准化后的形状不变\n",
        "            y.set_shape( x.get_shape())\n",
        "            return y\n",
        "\n",
        "def main ():\n",
        "    tf.reset_default_graph ()\n",
        "    with tf.Session() as sess :\n",
        "        tf.set_random_seed (1)  # tf中的随机种子\n",
        "        print(\" Begin to solve Allen - Cahn equation \")\n",
        "        model = SolveAllenCahn (sess)  # 创建对象\n",
        "        model.build()   # 调用对象方法，构建了一个模型，即定义了各个解，但没有传入数据\n",
        "        model.train ()  # 生成并传数据到build\n",
        "        output = np.zeros ((len(model.init_history), 3))   # 初始化结果为0,后面填充\n",
        "        output[:,0] = np.arange(len( model.init_history )) \\\n",
        "                             * model.n_displaystep         # 输出step\n",
        "        output[:,1] = model.loss_history  # 输出loss列表\n",
        "        output[:,2] = model.init_history # 输出 Y0列表\n",
        "        np.savetxt(\"./ AllenCahn_d100.csv \" ,  # 保存输出结果\n",
        "                     output ,\n",
        "                     fmt =[ '%d', '%.5e', '%.5e'] ,\n",
        "                     delimiter =\",\",\n",
        "                     header =\"step ,loss function , \" + \\\n",
        "                     \" target value , runtime \" ,\n",
        "                     comments = '')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "        np.random.seed(1) # 定义一个随机数种子\n",
        "        for i in range(5):\n",
        "            print(str(i)+' run:')\n",
        "            main()  # 运行主程序"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6158e-01 , Y0 :  5.4381e-01 , runtime :   10 \n",
            "step :  1000 , loss : 1.1286e-02 ,  Y0 :  1.9333e-01 , runtime :  109 \n",
            "step :  2000 , loss : 3.8380e-04 ,  Y0 :  6.9254e-02 , runtime :  201 \n",
            "step :  3000 , loss : 2.3479e-04 ,  Y0 :  5.3180e-02 , runtime :  293 \n",
            "step :  4000 , loss : 1.9822e-04 ,  Y0 :  5.3185e-02 , runtime :  382 \n",
            " running time :  382.833 s \n",
            "1 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.5930e-01 , Y0 :  5.4381e-01 , runtime :   10 \n",
            "step :  1000 , loss : 1.1439e-02 ,  Y0 :  1.9381e-01 , runtime :  104 \n",
            "step :  2000 , loss : 4.1777e-04 ,  Y0 :  6.9284e-02 , runtime :  195 \n",
            "step :  3000 , loss : 2.6188e-04 ,  Y0 :  5.3316e-02 , runtime :  287 \n",
            "step :  4000 , loss : 2.2518e-04 ,  Y0 :  5.3059e-02 , runtime :  379 \n",
            " running time :  379.859 s \n",
            "2 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6276e-01 , Y0 :  5.4381e-01 , runtime :   11 \n",
            "step :  1000 , loss : 1.1409e-02 ,  Y0 :  1.9348e-01 , runtime :  112 \n",
            "step :  2000 , loss : 4.2574e-04 ,  Y0 :  6.9261e-02 , runtime :  207 \n",
            "step :  3000 , loss : 2.8871e-04 ,  Y0 :  5.3261e-02 , runtime :  303 \n",
            "step :  4000 , loss : 2.4900e-04 ,  Y0 :  5.3025e-02 , runtime :  396 \n",
            " running time :  396.468 s \n",
            "3 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6503e-01 , Y0 :  5.4381e-01 , runtime :   11 \n",
            "step :  1000 , loss : 1.1926e-02 ,  Y0 :  1.9363e-01 , runtime :  108 \n",
            "step :  2000 , loss : 4.3191e-04 ,  Y0 :  6.9401e-02 , runtime :  203 \n",
            "step :  3000 , loss : 2.3905e-04 ,  Y0 :  5.3344e-02 , runtime :  296 \n",
            "step :  4000 , loss : 2.1006e-04 ,  Y0 :  5.2704e-02 , runtime :  392 \n",
            " running time :  392.186 s \n",
            "4 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.5728e-01 , Y0 :  5.4381e-01 , runtime :   11 \n",
            "step :  1000 , loss : 1.1281e-02 ,  Y0 :  1.9379e-01 , runtime :  108 \n",
            "step :  2000 , loss : 4.0593e-04 ,  Y0 :  6.9413e-02 , runtime :  204 \n",
            "step :  3000 , loss : 2.3317e-04 ,  Y0 :  5.3447e-02 , runtime :  298 \n",
            "step :  4000 , loss : 2.0853e-04 ,  Y0 :  5.2984e-02 , runtime :  393 \n",
            " running time :  394.024 s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCC38hGsJwWU",
        "colab_type": "text"
      },
      "source": [
        "### 四、BN\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdzAAgAlJwWV",
        "colab_type": "text"
      },
      "source": [
        "#### tf.nn.moments输出就是BN需要的mean和variance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVJSrc34JwWW",
        "colab_type": "code",
        "colab": {},
        "outputId": "edb9d61a-1a48-4d19-cceb-6e39d72363fc"
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.InteractiveSession()\n",
        "img = tf.random_normal([2, 3])\n",
        "axis = list(range(len(img.get_shape()) - 1))\n",
        "mean, variance = tf.nn.moments(img, axis)\n",
        "img.eval(),mean.eval(),variance.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 1.2752138 , -1.3859133 ,  0.44791156],\n",
              "        [-0.2962764 , -0.01701711,  0.16417223]], dtype=float32),\n",
              " array([ 1.368106  ,  0.68490934, -0.598477  ], dtype=float32),\n",
              " array([0.07034072, 1.6729448 , 0.6175715 ], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_HmxGghJwWZ",
        "colab_type": "text"
      },
      "source": [
        "#### assign_moving_average() 移动平均"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfM5WAKZJwWa",
        "colab_type": "text"
      },
      "source": [
        "assign_moving_average(variable, value, decay, zero_debias=True, name=None)<br>\n",
        "其实内部计算比较简单，公式表达如下：<br>\n",
        "variable = variable * decay + value * (1 - decay)"
      ]
    }
  ]
}