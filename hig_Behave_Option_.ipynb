{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hig_Behave_Option .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/small-qiu/Deep_learning/blob/master/hig_Behave_Option_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfBAAmsUTq6r"
      },
      "source": [
        "## 一、基本设置，搭建环境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B1ERE42oXQv",
        "outputId": "7fa4b87f-5582-49d8-9d52-3ab181fecf25"
      },
      "source": [
        "# 连接GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "     raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VnySDHPbImS",
        "outputId": "d425037c-2fab-4690-f2ce-d4003bbc2a4b"
      },
      "source": [
        "# 导入数据\n",
        "# 挂载云端硬盘\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CNncRAcbMwD"
      },
      "source": [
        "# 读取文件\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/Heston_model/')  # 已经进入目录下"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbxTqiFNUV-r"
      },
      "source": [
        "# 二、 期权定价类函数"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "58gOklnLpCgM",
        "outputId": "ec9d714b-6c25-48ea-c9e1-cf821febc406"
      },
      "source": [
        "import time  \n",
        "import math\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average   # 移动平均\n",
        "from scipy.stats import multivariate_normal as normal            # 产生正态分布随机数\n",
        "from tensorflow.python.ops import control_flow_ops               #用于控制流  \n",
        "from tensorflow import random_normal_initializer as norm_init    #生成具有正态分布的张量的初始化器\n",
        "from tensorflow import random_uniform_initializer as unif_init   #生成具有均匀分布的张量的初始化器\n",
        "from tensorflow import constant_initializer as const_init        #生成具有常量值的张量的初始化器\n",
        "tf.disable_v2_behavior()  # 兼容tensorflow 1.14\n",
        "\n",
        "class Option_SVM(object):\n",
        "    def __init__ (self,sess,M,Yini,X_train,C_endt,X_val,C_endv,maxstep,printstep,learning_rate):\n",
        "        self.sess = sess   # 会话\n",
        "        self.M = M \n",
        "        self.d = 2 * self.M      # 数据的维度\n",
        "        self.T = 400.0/250      # 每一条路径的时间长度\n",
        "        # parameters for the algorithm\n",
        "        self.dt = 1/250    # 每一个路径中数据时间间隔，delta t = 1/32\n",
        "        self.n_time = int(self.T/self.dt)     # 有20个t\n",
        "        self.n_layer = 4    # 神经网络的层数\n",
        "        self.n_neuron = [self.d ,self.d+10,self.d+10, self.d]    # 各层神经元的个数，对应输入、隐藏层1、隐藏层2、输出\n",
        "        self.batch_size = 128     # 一次用到个路径计算，64*4=256\n",
        "        self.valid_size =256     # 256个蒙特卡洛样本（路径）\n",
        "        self.n_maxstep = maxstep     # 迭代步数\n",
        "        self.n_displaystep = printstep     # 每*步print一次\n",
        "        self.learning_rate = learning_rate    # 学习率\n",
        "        self.Yini = Yini      # 初始值Y0的最大最小值\n",
        "        self.X_train = X_train   # 训练集X\n",
        "        self.C_endt = C_endt     # 训练集的Y\n",
        "        self.X_val = X_val       # 验证集的X\n",
        "        self.C_endv = C_endv     # 验证集的Y\n",
        "        \n",
        "        # some basic constants and variables\n",
        "        self.sqrth = math.sqrt(self.dt)    # 根号delta t，后面用于计算\n",
        "        self.t_stamp = np.arange(0,self.n_time)*self.dt  # 时间戳，累计的时间\n",
        "        self._extra_train_ops = []  # batch移动平均值操作，其中需要额外训练的beta和gamma\n",
        "        \n",
        "        self.c = 0.1\n",
        "        self.sigma = 0.1\n",
        "        self.gamma = 0.7\n",
        "        self.r = 0.03\n",
        "        self.rho = -0.9\n",
        "        self.paths = 10000    ## 所用总路径数\n",
        "        self.epoch_splits = int(self.paths/self.batch_size) - 1 \n",
        "        \n",
        "    def train(self):\n",
        "        # 主要函数，用于神经网络的训练\n",
        "        start_time = time.time()   # 起始时间\n",
        "        self.global_step = \\\n",
        "            tf.get_variable('global_step', [] ,\n",
        "                              initializer = tf.constant_initializer(1),\n",
        "                              trainable = False,dtype = tf.int32 )   # 没有添加到要训练的变量列表，计步器\n",
        "        trainable_vars = tf.trainable_variables()  # 查看可训练的变量\n",
        "        grads = tf.gradients(self.loss,trainable_vars)  # loss可训练变量的梯度\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)    # 梯度优化器\n",
        "        apply_op = optimizer.apply_gradients(zip(grads,trainable_vars) ,    # 将梯度用来更新trainable_vars列表中的东西\n",
        "                                          global_step = self.global_step)   # 更新梯度和迭代次数\n",
        "        \n",
        "        train_ops = [apply_op] + self._extra_train_ops   # 添加操作，相当于list1.extand(list2)\n",
        "        self.train_op = tf.group(* train_ops)   # tf.group(*train_ops)组合*train_ops的操作\n",
        "        \n",
        "        self.loss_history = []   # 用于记录loss值\n",
        "        self.init_history = []   # 用于记录Y0的值\n",
        "        \n",
        "        # for validation,256条蒙特卡洛做验证集\n",
        "        dW_valid , X_valid,c_endv = self.sample_path(self.valid_size,0,tra=False)   # 生成数据\n",
        "        feed_dict_valid = { self.dW : dW_valid,   # 喂数据给buildmodel中的占位符\n",
        "                            self.X : X_valid,\n",
        "                            self.C_end : c_endv,\n",
        "                            self.is_training: False}   # 不列入迭代范围\n",
        "        # initialization\n",
        "        step = 1\n",
        "        self.sess.run (tf.global_variables_initializer())  # 初始化全局变量\n",
        "        # 运行框架,没有训练时的loss\n",
        "        temp_loss = self.sess.run(self.loss ,feed_dict = feed_dict_valid )  # 计算损失       \n",
        "        temp_init = self.Y0.eval()[0] # # 取出值，Y0是二维张量\n",
        "        self.loss_history.append(temp_loss)  # 记录loss\n",
        "        self.init_history.append(temp_init)  # 记录Y0\n",
        "        print(\"step : %5u , loss : %.4e , \" % (0 ,temp_loss ) + \"Y0 : % .4e , runtime : %4u \" % \\\n",
        "              (temp_init, time.time()-start_time + self.t_bd))\n",
        "        \n",
        "        # begin sgd iteration，0-4000步\n",
        "        for i in range (self.n_maxstep +1):   \n",
        "            step = self.sess.run (self.global_step)\n",
        "            dW_train,X_train,c_endt = self.sample_path(self.batch_size,i,tra=True)  # 生成数据\n",
        "            self.sess.run(self.train_op,\n",
        "                          feed_dict ={self.dW : dW_train ,   # 喂数据给buildmodel中的占位符\n",
        "                                      self.X : X_train ,\n",
        "                                      self.C_end:c_endt,\n",
        "                                      self.is_training : True })\n",
        "            if step % self.n_displaystep == 0:   # 每100步用验证集测试一下损失和Y0的值\n",
        "                temp_loss = self.sess.run(self.loss ,\n",
        "                                          feed_dict = feed_dict_valid)\n",
        "                temp_init = self.Y0.eval()   # 取出值,print\n",
        "                self.loss_history.append(temp_loss)    # 损失值，最后输出\n",
        "                self.init_history.append(temp_init)   # Y0值，Y0是二维张量\n",
        "                print(\"step : % 5u , loss : %.4e , \" % \\\n",
        "                        ( step , temp_loss ) + \\\n",
        "                        \" Y0 : % .4e , runtime : %4u \" % \\\n",
        "                        (temp_init , time.time() - start_time + self.t_bd ))\n",
        "            step += 1\n",
        "        end_time = time.time()  # 训练结束的总时间\n",
        "        print(\" running time : % .3f s \" % \\\n",
        "                ( end_time - start_time + self.t_bd ))\n",
        "\n",
        "    def build(self):\n",
        "        # build the whole network by stacking subnetworks，构架大网络\n",
        "        start_time = time.time () \n",
        "        # dW、X、is_training的占位符，为什么是None,因为一次计算一个batch\n",
        "        self.dW = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time-1] ,name = 'dW')   # None*100*20\n",
        "        self.X = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time] ,name = 'X')   # None*100*20\n",
        "        self.C_end = tf.placeholder(tf.float32 ,[ None ,1] ,name = 'C_end') # C_end\n",
        "        self.is_training = tf.placeholder (tf.bool)\n",
        "        \n",
        "        # 初始化Y0\\Z0\n",
        "        self.Y0 = tf.Variable(tf.random_uniform([1],                      # u0初始化,一个维度一个值  \n",
        "                            minval = self.Yini [0] ,   # 最小值 \n",
        "                            maxval = self.Yini [1] ,   # 最大值\n",
        "                            dtype = tf.float32 ));\n",
        "        self.Z0 = tf.Variable (tf.random_uniform ([1,self.d] ,    # u梯度的初始值，一个1*d 向量\n",
        "                            minval = -.1 ,   # 最小值\n",
        "                            maxval =.1 ,    # 最大值\n",
        "                            dtype = tf.float32 ))\n",
        "        self.allones = tf.ones(shape = tf.stack([tf.shape(self.dW)[0],1]) ,   # tf.shape(self.dW)[0]=len(batch),shape=(batch,1)\n",
        "                         dtype = tf.float32 )                        # 作用，批量（batch）产生初始值\n",
        "        \n",
        "\n",
        "        Y = self.allones * self.Y0  # 初始的Y作为输入,每一个batch都赋予相同的初始Y值，Y是一个(batch,1)二维矩阵[[],[],..,]\n",
        "        Z = tf.matmul(self.allones, self.Z0 )  # 初始的Z做为输入，作用和Y相同，但是由于Z是一个向量所以要乘积（batch,d）矩阵\n",
        "        ww = tf.ones(shape =(M,1),dtype = tf.float32)/M\n",
        "        \n",
        "        with tf.variable_scope('forward'):   # 前向\n",
        "            for t in range(self.n_time-2):  # 前N-2个xt的网络\n",
        "                    Y = self.f_tf(self.t_stamp[t],Y,Z,self.X[:,:,t],self.dW[:,:,t])  # 递推公式     \n",
        "                    Z = self._one_time_net(self.X[:,:,t+1] ,  # 得到“u梯度”-- 由神经网络训练而来的。\n",
        "                                       str(t +1))  \n",
        "            # terminal time，因为最后一刻的Y不用神经网络了\n",
        "            Y = self.f_tf(self.t_stamp[self.n_time-1] ,Y,Z,self.X[:,:, -2],self.dW[:,:, -1])   # X最后一个用于标的\n",
        "            Y = tf.matmul(Y,ww)\n",
        "            term_delta = Y- tf.matmul(self.C_end,ww) # 损失函数公式\n",
        "            self.clipped_delta = tf.clip_by_value(term_delta ,-50.0 , 50.0)  # 截断\n",
        "            self.loss = tf.reduce_mean(self.clipped_delta**2)+0.01*(self.Y0)**2#计算损失，来源于损失函数\n",
        "        self.t_bd = time.time() - start_time  # 生成网络的时间\n",
        "\n",
        "\n",
        "    def sample_path(self, n_sample,s,tra=True):\n",
        "        dW = np.zeros([n_sample,self.d,self.n_time-1])   # 少一时刻，0时刻\n",
        "        for i in range(n_sample):\n",
        "            z11 = np.random.randn(self.n_time-1)*self.sqrth\n",
        "            z12 = np.random.randn(self.n_time-1)*self.sqrth\n",
        "            w1 = z11\n",
        "            w2 = self.rho*z11+np.sqrt(1-self.rho**2)*z12\n",
        "            dW[i,0]=w1\n",
        "            dW[i,1]=w2\n",
        "        if tra:\n",
        "            # 提取训练数据\n",
        "            if s == 10000:\n",
        "                random.shuffle(X_train)\n",
        "            s = s % self.epoch_splits\n",
        "            X = X_train[s*n_sample:(s+1)*n_sample]\n",
        "            c_end = c_endt[s*n_sample:(s+1)*n_sample]\n",
        "        else:\n",
        "            # 提取验证数据\n",
        "            X = X_val\n",
        "            c_end = c_endv\n",
        "        return dW,X,c_end\n",
        "\n",
        "    def f_tf(self,t,Y,Z,X,dW):   # 递推公式   \n",
        "        f = tf.reshape((Z[:,:M]*(self.sigma/(1-self.gamma*X[:,M:]))*dW[:,:M]+\\\n",
        "                        Z[:,M:]*self.c*tf.sqrt((1-X[:,M:])*X[:,M:])*dW[:,M:]),shape=(-1,M))# @@@@@@@\n",
        "        return Y+self.r*Y*self.dt+f\n",
        "\n",
        "    def _one_time_net(self , x ,name):\n",
        "        # x.shape = (batch,2)，得到z.shape=(batch,2)。输入的数据含有t,当t确定x就是二维的\n",
        "        # 一个batch在t时刻的网络构架，输出梯度,不用\n",
        "        with tf.variable_scope(name):\n",
        "            x_norm = self._batch_norm(x, name = 'layer0_normal')  # 对batch标准化，作为输入\n",
        "            layer1 = self._one_layer(x_norm , self.n_neuron [1] ,   # 隐藏层1输入输出input(batch,d),output(batch，d+10)\n",
        "                                      name = 'layer1')\n",
        "            layer2 = self._one_layer(layer1,self.n_neuron[2] ,  # 隐藏层2 input(batch,d+10),output(batch,d+10)\n",
        "                                      name = 'layer2')\n",
        "            z = self._one_layer(layer2 , self.n_neuron [3] , #  输出层，不加relu函数做激活input(batch,d+10),output(baatch,d)\n",
        "                                     activation_fn = None , name = 'final')\n",
        "        return z\n",
        "\n",
        "    def _one_layer(self , input_ , out_sz ,activation_fn = tf.nn.relu ,std =5.0 , name = 'linear'):\n",
        "        with tf.variable_scope(name):\n",
        "            shape = input_.get_shape().as_list()\n",
        "            w = tf.get_variable('Matrix',    \n",
        "                                [shape[1], out_sz] ,tf.float32,   \n",
        "                                norm_init(stddev = \\\n",
        "                                          std / np.sqrt(shape[1]+ out_sz ))) \n",
        "            hidden = tf.matmul(input_ ,w)  \n",
        "            hidden_bn = self._batch_norm(hidden, name = 'normal')      \n",
        "        if activation_fn != None :\n",
        "            return activation_fn(hidden_bn)  # 激活函数\n",
        "        else :\n",
        "            return hidden_bn  #不加激活函数,线性,最后一层网络\n",
        "\n",
        "    def _batch_norm(self , x , name ):\n",
        "        \"\"\" Batch normalization \"\"\" # beta、gamma需要训练，第三类参数来源,一次标准化需要2列参数\n",
        "        with tf.variable_scope(name):\n",
        "            params_shape = [x.get_shape()[ -1]]   # [d,d+10,d+10,d]，第一个维度是batch\n",
        "            beta = tf.get_variable('beta', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         norm_init(0.0 , stddev =0.1 ,\n",
        "                                         ))\n",
        "            gamma = tf.get_variable( 'gamma', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         unif_init (0.1,0.5 ,\n",
        "                                          ))\n",
        "            mv_mean = tf.get_variable('moving_mean' ,   # 由于每次的batch不同，所以用moving_mean来改进mean\n",
        "                                         params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         const_init (0.0) ,\n",
        "                                         trainable = False )\n",
        "            mv_var = tf.get_variable('moving_variance' ,\n",
        "                                        params_shape ,\n",
        "                                        tf.float32 ,\n",
        "                                        const_init(1.0) ,\n",
        "                                        trainable = False )\n",
        "            \n",
        "            # These ops will only be preformed when training\n",
        "            mean ,variance = tf.nn.moments(x ,[0] , name = 'moments')#需要标准化的中心维度,[0]表示batch,求64个数据的均值方差\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_mean , mean , 0.99))  # 下面详解\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_var , variance , 0.99))\n",
        "            \n",
        "            mean,variance = \\\n",
        "                control_flow_ops.cond(self.is_training ,            # control_flow_ops.cond控制执行流，第一个为条件\n",
        "                                     lambda :( mean , variance ) , # 条件True时执行，train时，需要进行重新求均值方差\n",
        "                                     lambda :( mv_mean , mv_var )) # 条件False时执行,test时，直接调用最后一次的平滑值\n",
        "            \n",
        "            y = tf.nn.batch_normalization (x , mean , variance ,\n",
        "                                           beta , gamma , 1e-6)   \n",
        "            # 上面一步的操作相当于:  \n",
        "            # y = (y - mean)/tf.sqrt(variance+1e-6)  # 1e-6 epslion\n",
        "            # y = y * gamma + beta\n",
        "            \n",
        "            # 确保标准化后的形状不变\n",
        "            y.set_shape( x.get_shape())\n",
        "            return y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhQbrjoLUsbR"
      },
      "source": [
        "# 三、数据导入与构造"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vegBzZGTU2vi"
      },
      "source": [
        "#### 3.1 可修改变量 —— M,*资产个数*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqQUD0bpLBBN"
      },
      "source": [
        "# 构造和50w相同的初始S0和V0,和蒙特卡洛一样\n",
        "xchi0 = pd.read_excel(r'x0_chi01.xlsx')\n",
        "x0 = np.array(xchi0['x0'])\n",
        "chi0 = np.array(xchi0['chi0'])\n",
        "\n",
        "# 基本参数设置\n",
        "T =200.0/250 \n",
        "dt = 1/250\n",
        "D = int(T/dt)\n",
        "a = 0.95\n",
        "b = 0.8\n",
        "c = 0.1\n",
        "sigma = 0.1\n",
        "rho = -0.9\n",
        "gamma = 0.7 \n",
        "r = 0.03  # 无风险利率\n",
        "K = 100 # 标的价格\n",
        "#####################\n",
        "#####################\n",
        "M = 400 # 资产个数\n",
        "\n",
        "\n",
        "def paths50d(num):\n",
        "    # 生成数据 S.shape = (num,D*M)\n",
        "    # 一次性生成路径\n",
        "    x = np.zeros([num,D*M])\n",
        "    chi = np.zeros([num,D*M])\n",
        "    w = np.zeros([M,1])\n",
        "\n",
        "    for i in range(M):\n",
        "        \n",
        "        if M == 1:\n",
        "          # 计算单个资产的情形，与论文的设置相同\n",
        "          chi[:,i*D] = 0.93\n",
        "          x[:,i*D] = np.log(100)\n",
        "        else:\n",
        "          chi[:,i*D] = chi0[i]\n",
        "          x[:,i*D] = x0[i]\n",
        "        w[i,0] = 1/M\n",
        "    for k in  range(M):\n",
        "        for i in  range(1,D):    \n",
        "            j = k*D + i\n",
        "            W1 = np.random.randn(num)\n",
        "            W1_5 = np.random.randn(num)\n",
        "            W2 = W1*rho + W1_5*np.sqrt(1-rho**2)\n",
        "            x[:,j] = x[:,j-1] +(r-0.5*pow(sigma/(1-gamma*chi[:,j-1]),2))*dt+sigma/(1-gamma*chi[:,j-1])*np.sqrt(dt)*W1\n",
        "            chi[:,j] = chi[:,j-1] + b*(a-chi[:,j-1])*dt+c*np.sqrt((1-chi[:,j-1])*chi[:,j-1])*np.sqrt(dt)*W2  \n",
        "\n",
        "    # 下面做蒙特卡洛仿真\n",
        "    x_end =  np.zeros([num,M])\n",
        "    for i in range(M):\n",
        "        # 提取做好T时刻的值\n",
        "        x_end[:,i] = x[:,(i+1)*D-1]\n",
        "\n",
        "    c_end = np.zeros([num,1])  # payoff值\n",
        "    for i in range(num):\n",
        "        c_end[i] = [max(h-K,0) for h in np.dot(np.exp(x_end[i,:]),w)]\n",
        "    cmc = np.mean(c_end)*np.exp(-r*T)  # 蒙特卡洛估计值\n",
        "    x = np.reshape(x,(num,M,D))      \n",
        "    chi = np.reshape(chi,(num,M,D))   \n",
        "    return x,chi,cmc,c_end\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaLFUpOqU_4d"
      },
      "source": [
        "#### 3.2 50w 蒙特卡洛估计值"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdX3j1rfLJ-6",
        "outputId": "5fb0b64b-6d77-41b9-85fe-eb215b7b344b"
      },
      "source": [
        "\n",
        "## 50w 条数据计时器**** rework\n",
        "import time\n",
        "time_start=time.time()\n",
        "x50w,chi50w,c50w,c_endt50w = paths50d(500000)\n",
        "time_end=time.time()\n",
        "print('time cost',time_end-time_start,'s')\n",
        "print(c50w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time cost 150.10790276527405 s\n",
            "0.3951372923345897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AEgB9GrVFno"
      },
      "source": [
        "#### 3.3 构造输入数据"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XACksGSRm1h1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAtKUpdEpO2A"
      },
      "source": [
        "np.random.seed(30)\n",
        "# 得到10000条训练数据\n",
        "\n",
        "numtrain = 10000\n",
        "S,V,c1,c_endt = paths50d(numtrain)\n",
        "X_train = np.zeros([numtrain,2*M,D])     \n",
        "for i in range(numtrain):\n",
        "    X_train[i,:] = np.vstack((S[i,:],V[i,:]))\n",
        "print(X_train.shape)#数据读入和预处理,读取数据较慢\n",
        "# c_end是最后的max(st,w-k,0),可作为loss的pre\n",
        "\n",
        "\n",
        "# 验证数据 256 条数据\n",
        "numval = 256\n",
        "st_val,vt_val,c2,c_endv = paths50d(numval)\n",
        "X_val = np.zeros([numval,2*M,D])\n",
        "for i in range(numval):\n",
        "    X_val[i,:] = np.vstack((st_val[i,:],vt_val[i,:]))\n",
        "print(X_val.shape)\n",
        "print(c_endt.shape,c_endv.shape)\n",
        "print(f'1000条蒙特卡洛：{c1}\\n     256条验证：{c2}')\n",
        "# print(f'   50w蒙特卡洛: {c50w}')\n",
        "print('数据构造完成！')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn5Xd9x1VWyF"
      },
      "source": [
        "#### 3.4 最终的期权定价\n",
        "##### 类参数需修改:\n",
        "M  : 资产个数<br>\n",
        "[] : 初值范围<br>\n",
        "-3位置参数 : 迭代步数<br>\n",
        "-2位置参数 ： 每*步打印一次<br>\n",
        "-1参数设置 ： 学习率\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyKzMgbwURn4",
        "outputId": "d7a9809a-ad66-4d9a-ca9d-bebfcbfe9e6a"
      },
      "source": [
        "def main ():\n",
        "    # 运行框架启动，不用修改\n",
        "    tf.reset_default_graph ()\n",
        "    with tf.Session() as sess :\n",
        "        tf.set_random_seed (1)  # tf中的随机种子\n",
        "        print(\" 期权定价 :\")\n",
        "        \n",
        "        ########################--------期权类参数-------################################\n",
        "        model = Option_SVM (sess,200,[0.2,0.6],X_train,c_endt,X_val,c_endv,700,30,4e-4)  # 创建对象\n",
        "        #################################################################################\n",
        "        print('开始构建网络')\n",
        "        model.build()   # 调用对象方法，构建了一个模型，即定义了各个解，但没有传入数据\n",
        "        print('网络构建完成')\n",
        "        model.train()  # 生成并传数据到build\n",
        "        \n",
        "        \n",
        "        # 结果保存,不用修改\n",
        "        output = np.zeros ((len(model.init_history), 3))   # 初始化结果为0,后面填充\n",
        "        output[:,0] = np.arange(len( model.init_history ))* model.n_displaystep         # 输出step\n",
        "        output[:,1] = model.loss_history  # 输出loss列表\n",
        "        output[:,2] = model.init_history # 输出 Y0列表\n",
        "        np.savetxt(\"./reworkd2.csv \" ,  # 保存输出结果\n",
        "                     output ,\n",
        "                     fmt =[ '%d', '%.5e', '%.5e'] ,\n",
        "                     delimiter =\",\",\n",
        "                     header =\"step,loss function,target value\" ,\n",
        "                     comments = '')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "        np.random.seed(1) # 定义一个随机数种子          \n",
        "        main()  # 运行主程序 "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 期权定价 :\n",
            "开始构建网络\n",
            "网络构建完成\n",
            "step :     0 , loss : 4.6931e+00 , Y0 :  5.2508e-01 , runtime :  186 \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}